{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a731cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib.image import imread, imsave\n",
    "from PIL import Image\n",
    "\n",
    "import torch                                                \n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e712fa",
   "metadata": {},
   "source": [
    "### 0) Hyperparmeters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00399277",
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_workers = 1\n",
    "batch_size = 128     # NCHW 중 N값\n",
    "nc = 3  # the number of output channels\n",
    "nz = 100  # size of z latent vector(랜덤개수) (i.e., size of the generator input)\n",
    "ngf = 64  # size of feature maps in generator\n",
    "ndf = 64  # size of feature maps in discriminator\n",
    "epochs = 50     #\n",
    "lr = 0.0002     #업데이트 간격\n",
    "beta1 = 0.5     # optima에 들어갈 값\n",
    "seed = 230522   # 랜덤값 고정하기 위한\n",
    "torch.manual_seed(seed)\n",
    "check_epoch = 1\n",
    "num_data = 50000    #202599중 5만개  # whole data\n",
    "\n",
    "save_dir = \"save_HT\"\n",
    "os.makedirs(save_dir)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5268b8",
   "metadata": {},
   "source": [
    "### 1) Define generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bf52444",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, (4, 4), 1, 0, bias=False),  # (100, 1, 1) -> (512, 4, 4) / can be linear  \n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),    #  activation func\n",
    "            \n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, (4, 4), 2, 1, bias=False),  # (512,4,4) -> (256, 8, 8)\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, (4, 4), 2, 1, bias=False),  #  (256, 8, 8) -> (128, 16, 16)\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, (4, 4), 2, 1, bias=False),  # (128, 16, 16) -> (64, 32, 32)\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(ngf, nc, (4, 4), 2, 1),  # (3, 64, 64)  # 이미지 채널에 맞게 3으로 줄여나가고 / 정보보존\n",
    "            # nn.BatchNorm2d(nc),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):           \n",
    "        \"\"\"z: random uniform (or Gaussian) noise of size (N, 100, 1, 1)\"\"\"\n",
    "        return self.main(z)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # init이 일단 실행 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b433e0e6",
   "metadata": {},
   "source": [
    "### 2) Define discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a3fbfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(nc, ndf, (4, 4), 2, 1),  # (64, 32, 32)\n",
    "            # nn.BatchNorm2d(ndf),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            \n",
    "            nn.Conv2d(ndf, ndf * 2, (4, 4), 2, 1, bias=False),  # (128, 16, 16)\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            \n",
    "            nn.Conv2d(ndf * 2, ndf * 4, (4, 4), 2, 1, bias=False),  # (256, 8, 8)\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            \n",
    "            nn.Conv2d(ndf * 4, ndf * 8, (4, 4), 2, 1, bias=False),  # (512, 4, 4)\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            \n",
    "            nn.Conv2d(ndf * 8, 1, (4, 4), 1, 0),  # (512, 4, 4) -> (1, 1, 1) / can be linear\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.main(inputs).view(-1)  # (N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7626ef94",
   "metadata": {},
   "source": [
    "### 3) Define GAN loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5aba1278",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()  # binaty Cross Entropy / class가 2개"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe178fd3",
   "metadata": {},
   "source": [
    "### 4) Module construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2652e0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02) # 가우시안에서 평균이0\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a5be84e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (main): Sequential(\n",
       "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (13): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = Generator()\n",
    "g.apply(weights_init) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7047f5de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (main): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (12): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = Discriminator()\n",
    "d.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21f0edc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_optim = torch.optim.Adam(\n",
    "    g.parameters(),  #학습시킬 파라미터\n",
    "    lr=lr,\n",
    "    betas=(beta1, 0.999),\n",
    "    weight_decay=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72b0fb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_optim = torch.optim.Adam(\n",
    "    d.parameters(),\n",
    "    lr=lr,\n",
    "    betas=(beta1, 0.999),\n",
    "    weight_decay=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77b3df6",
   "metadata": {},
   "source": [
    "### 5) Data construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfd9b9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  py에서 대문자는 클래스  파이토치 클래스 기본구조 : 3 method\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f98d048",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Face(Dataset):\n",
    "    def __init__(self, datadir, num_data, transforms=[]):\n",
    "        files = os.listdir(datadir)                                         # 폴더안의 파일들을 리스트 형식으로 \n",
    "        files = sorted(files, key=lambda x: int(x[:-4]))                   #정렬함수 sorted \n",
    "        self.files = [os.path.join(datadir, file) for file in files]\n",
    "        self.files = self.files[:num_data]  #num_data = 50000 / 5만개 까지의 이미지만 사용\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):    \n",
    "        \"\"\"\n",
    "        Face dataset contains 202599 items (N=202599, C=3, H=218, W=178)\n",
    "        Among them, uses specific number of data only (num_data parameter in __init__)\n",
    "        \"\"\"\n",
    "        return len(self.files) ##전체 데이터 갯수\n",
    "\n",
    "    def __getitem__(self, item):               #HDD -> RAM으로 전처리해서 데이터 이동\n",
    "        file = self.files[item]\n",
    "        img = imread(file)                      # 실제로 이미지 읽는 함수 : imread\n",
    "        for transform in self.transforms:      \n",
    "            img = transform(img)\n",
    "        return img\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f99163b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kimhyuntae\\AppData\\Local\\Temp\\ipykernel_26216\\3668337002.py:12: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  transforms.Resize(size=(64, 64), interpolation=Image.BICUBIC),\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 지정된 경로를 찾을 수 없습니다: 'img_align_celeba'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26216\\3668337002.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m dataset = Face(\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;34m\"img_align_celeba\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mnum_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_26216\\565490940.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, datadir, num_data, transforms)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mFace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatadir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mfiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatadir\u001b[0m\u001b[1;33m)\u001b[0m                                         \u001b[1;31m# 폴더안의 파일들을 리스트 형식으로\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mfiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m                   \u001b[1;31m#정렬함수 sorted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatadir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 지정된 경로를 찾을 수 없습니다: 'img_align_celeba'"
     ]
    }
   ],
   "source": [
    "def max_min_scaling(tensor):         #정규화\n",
    "    u = tensor.min()\n",
    "    v = tensor.max()\n",
    "    return (tensor - 0.5 * (v + u)) / (0.5 * (v - u))  # -1 ~ 1\n",
    "\n",
    "\n",
    "dataset = Face(\n",
    "    \"img_align_celeba\",\n",
    "    num_data=num_data,\n",
    "    transforms=[\n",
    "        transforms.ToPILImage(),  # Resize class only accepts PIL image as a forward input (torch 1.6.0)\n",
    "        transforms.Resize(size=(64, 64), interpolation=Image.BICUBIC),\n",
    "        transforms.ToTensor(),#파이토치에서 사용하는 데이터형식\n",
    "        max_min_scaling\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(            \n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    #num_workers=num_workers,  # multi-process dataloading\n",
    "    #pin_memory=True,\n",
    "    #sampler=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce685079",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)    #plt.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4251dcd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(dataloader)   # 50000/128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d0da29",
   "metadata": {},
   "source": [
    "### 6) Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6687f4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, g, d, criterion, g_optim, d_optim):\n",
    "    g.train()\n",
    "    d.train()\n",
    "\n",
    "    for batch_idx, imgs in enumerate(dataloader):                    \n",
    "        z = torch.FloatTensor(batch_size, nz, 1, 1).uniform_(-1, 1)  #랜덤값 \n",
    "\n",
    "        \n",
    "        # gen 시작\n",
    "        fake = g(z)\n",
    "        d_real = d(imgs)         # input실제이미지 output : 0~1\n",
    "        d_fake = d(fake.detach()) # input가짜이미지 output : 0~1\n",
    "        \n",
    "        d_loss_real = criterion(d_real, torch.ones_like(d_real))  # 실제이미지 넣고 lossfuc 1로보내기\n",
    "        d_loss_fake = criterion(d_fake, torch.zeros_like(d_fake))\n",
    "        d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
    "        d_optim.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optim.step()        #discri training\n",
    "\n",
    "        d_fake = d(fake)  # extends graph from generator to discriminator\n",
    "        g_loss = criterion(d_fake, torch.ones_like(d_fake))\n",
    "        g_optim.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optim.step()\n",
    "        \n",
    "        print('batch_idx: {}'.format(batch_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec15f32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(z, g, d):\n",
    "    g.eval()\n",
    "    d.eval()\n",
    "\n",
    "    with torch.no_grad():          #이미지 학습이 끝나면 discrimi \n",
    "        fake = g(z)\n",
    "\n",
    "        return denormalize(fake)\n",
    "    \n",
    "\n",
    "def denormalize(tensor):\n",
    "    return 0.5 * tensor + 0.5  # 0 ~ 1   -1~1를 0~1로 보내줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbe6f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AppendTensor:    #학습을\n",
    "    \"\"\"this class is for tensors to accumulate\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.sum = []\n",
    "\n",
    "    def update(self, tensor):\n",
    "        self.sum.append(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccc2e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure(epoch, size, tensor, save_dir):\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "    plt.title('epochs: {}'.format(epoch), fontsize=30)\n",
    "    plt.axis('off')\n",
    "    for i in range(size):\n",
    "        subplot = fig.add_subplot(10, 10, i + 1)\n",
    "\n",
    "        subplot.imshow(\n",
    "            tensor.permute(0, 2, 3, 1).numpy()[i]\n",
    "        )\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "\n",
    "    plt.savefig(\n",
    "        os.path.join(save_dir, \"{}.png\".format(epoch)),\n",
    "        dpi=300\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb310549",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fixed_z = torch.FloatTensor(100, nz, 1, 1).uniform_(-1, 1)\n",
    "# outs = AppendTensor()\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     train(dataloader, g, d, criterion, g_optim, d_optim)\n",
    "#     print('Epoch{} has been completed.'.format(epoch))\n",
    "#     print('')\n",
    "    \n",
    "#     if epoch % check_epoch == 0:\n",
    "#         fake = test(fixed_z, g, d)\n",
    "#         outs.update(fake)\n",
    "        \n",
    "#         figure(epoch, fixed_z.size(0), fake, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bb5c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(17, epochs):\n",
    "#     train(dataloader, g, d, criterion, g_optim, d_optim)\n",
    "#     print('Epoch{} has been completed.'.format(epoch))\n",
    "#     print('')\n",
    "    \n",
    "#     if epoch % check_epoch == 0:\n",
    "#         fake = test(fixed_z, g, d)\n",
    "#         outs.update(fake)\n",
    "        \n",
    "#         figure(epoch, fixed_z.size(0), fake, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baa87f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(28, epochs):\n",
    "#     train(dataloader, g, d, criterion, g_optim, d_optim)\n",
    "#     print('Epoch{} has been completed.'.format(epoch))\n",
    "#     print('')\n",
    "    \n",
    "#     if epoch % check_epoch == 0:\n",
    "#         fake = test(fixed_z, g, d)\n",
    "#         outs.update(fake)\n",
    "        \n",
    "#         figure(epoch, fixed_z.size(0), fake, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd5ebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(g.state_dict(), os.path.join(save_dir, \"g_state_dict_at_epoch{}.pth.tar\".format(epoch - 1)))\n",
    "# torch.save(d.state_dict(), os.path.join(save_dir, \"d_state_dict_at_epoch{}.pth.tar\".format(epoch - 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92690ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KMP_DUPLCATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264da46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.load_state_dict(torch.load(\"save\\g_state_dict_at_epoch48.pth.tar\"))\n",
    "#d.load_state_dict(torch.load(save_dir + \"\\d_state_dict_at_epoch27.pth.tar\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d67c6b",
   "metadata": {},
   "source": [
    "### 7) Walking in the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dac2da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lerp(v0, v1, n_step):\n",
    "    h = []\n",
    "    for i in torch.linspace(0, 1, n_step):\n",
    "        h.append((1 - i) * v0 + i * v1)\n",
    "    \n",
    "    return torch.cat(h, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03336a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    # define latent vectors\n",
    "    z0 = torch.Tensor(1, nz, 1, 1).uniform_(-1, 1)\n",
    "    z1 = torch.Tensor(1, nz, 1, 1).uniform_(-1, 1)\n",
    "    zs = lerp(z0, z1, 10)\n",
    "    #print(zs.size())  # torch.Size([10, 100, 1, 1])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        fake = g(zs)\n",
    "        fake = denormalize(fake)\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "    for i in range(fake.size(0)):\n",
    "        subplot = fig.add_subplot(1, 10, i+1)\n",
    "        subplot.imshow(fake[i].permute(1, 2, 0).numpy())\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2b4351",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mk = torch.Tensor(1, nz, 1, 1).uniform_(-1, 1)\n",
    "r_mk=denormalize(g(z_mk))\n",
    "plt.imshow(r_mk.detach().permute(0,2,3,1).numpy()[0])  #permute 파라미터 순서 바꿈\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f439bfda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
